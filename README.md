ğŸ¤– RAG-Based Question Answering System with LLaMA-2 & Mistral-7B



An advanced Retrieval-Augmented Generation (RAG) system using Hybrid Search, Conversational Memory, and Generative LLMs for high-accuracy question-answering. ğŸš€

ğŸ“Œ Features

âœ… Conversational Memory (Multi-Turn Chat)âœ… Hybrid Search (BM25 + FAISS) for Improved Retrievalâœ… Adaptive Chunking (1000 tokens + 200 overlap)âœ… ReRanker for Better Answer Selectionâœ… Instruction-Tuned Models for Higher Accuracyâœ… Cloud Deployment (Streamlit, Hugging Face Spaces, AWS)âœ… Fine-Tuning LLaMA-2 for Domain-Specific Q&A (Optional)

âš™ï¸ Installation

# Clone this repository
git clone https://github.com/yourusername/RAG-QA-System.git
cd RAG-QA-System

# Install dependencies
pip install -r requirements.txt

ğŸš€ Usage

Run the interactive chatbot:

python app.py

Or use the system in a Python script:

from your_rag_module import question_answer
user_query = "What is deep learning?"
response = question_answer({'question': user_query, 'context': retrieved_docs})
print(response['answer'])

ğŸ“¸ Screenshots



ğŸ“š How It Works

Retrieval ğŸ”: The system fetches relevant documents using BM25 + FAISS Hybrid Search.

ReRanking ğŸ“Š: Uses a ReRanker to select the best passages.

Generation âœ¨: Passes context + question to Mistral-7B or LLaMA-2-13B.

Response ğŸ“: Returns a human-like, context-aware answer.

ğŸ›  Technologies Used

LLMs: LLaMA-2-13B, Mistral-7B

Retrieval: FAISS, BM25, LangChain

Search Ranking: ReRanker

Backend: Python, Hugging Face Transformers

Deployment: Streamlit, Hugging Face Spaces, AWS

ğŸŒŸ Contributing

Want to improve this project? Feel free to submit a pull request!

ğŸ“œ License

This project is licensed under the MIT License.

ğŸŒ Connect with Me!



ğŸš€ Star this repository if you like it! â­

